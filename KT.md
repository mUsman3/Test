## Purpose of the Pipeline

The pipeline serves as a common destination for events generated by various monitoring tools such as Splunk, Maul, TSOM, Datadog, Prometheus, and others. It is designed to achieve the following:

- **Data Aggregation**: Collect events from diverse monitoring tools into a unified stream.
- **Data Normalization**: Standardize data formats to ensure consistency and compatibility.
- **Data Enrichment**: Enhance the raw data with additional context and information.

Once processed, the enriched and normalized data is sent to AIOps for advanced analytics and to ServiceNow for ticketing and incident management. This integration facilitates efficient monitoring, issue detection, and resolution workflows.

Before introducing the pipeline, the workflow was as follows: we collected events from different monitoring tools and sent them directly to ServiceNow. The events received on the ServiceNow side were in a raw format and included structures and fields that were not necessarily required. Additionally, every tool had its own way of sending events, each following a different structure. We were not using techniques like data aggregation, normalization, or enrichment with that approach, making it unsuitable for advanced analytics.

## Current State

The pipeline is designed to accommodate new event providers and scale as needed without requiring any fundamental changes. It normalizes events into a standard schema and sends them to the ITOM module in ServiceNow, which provides AIOps functionality.

Currently, the pipeline ingests events from Datadog, Maul, and Splunk. It has also been configured to receive events from Dynatrace and Cisco ThousandEyes, allowing these tools to send all events to the pipeline once they are ready. Updates to the pipeline have been successfully tested in both Test and Production environments, ensuring smooth integration and functionality.


## Working of the Pipeline

The pipeline is deployed on AWS Infrastructure and configured to operate in three environments: Dev, Test, and Prod. The services involved in completing the pipeline include API gateways, Lambda Functions, Dynamo DBs, SQS (Simple Queue Service), DLQs (Dead Letter Queue), and SNS (Simple Notification Service) Topics.

To illustrate the integration process, let's consider sending events from Cisco ThousandEyes to the pipeline for processing and subsequent forwarding to ServiceNow for management and analytics. We create integrations for this purpose. The pipeline is configured to function in each environment (Dev, Test, and Production), and integrations can be created accordingly.

For example, to test the pipeline in the Dev environment, we create an integration on Cisco ThousandEyes using the Dev URL, specifically configured for the Dev environment pipeline. Each environment (Dev, Test, Prod) has its own unique URL, enabling interaction with different pipelines tailored to each respective environment.

When events are forwarded to the pipeline through the integration we've set up, they first hit the API gateway

**Authentication** 

When a request is forwarded to the API gateway, it undergoes a series of checks to ensure its validity and authorization to interact with the pipeline resources. The API gateway is configured to verify if the request originates from the correct sources and if it has the necessary permissions to access the pipeline.

To authenticate the request, the API gateway calls upon an authorizer. In this scenario, Lambda Authorizer is utilized to validate the token associated with the request. If the token is correct and valid, the request is accepted; otherwise, it is rejected. As mentioned earlier, the inclusion of a token for authentication is imperative. This token is attached to the request during the integration setup process and is validated for every request received by the API gateway.

The Lambda Authorizer extracts the token from the request and queries it against DynamoDB. If the token is found in DynamoDB and associated with the correct application (e.g., ThousandEyes), and the source is valid, the request is permitted. Conversely, if the token is not found or is invalid, the request is rejected.

It's crucial to note that the token must be present in DynamoDB alongside the application it corresponds to, ensuring proper authorization. Once validated, the token grants permission to access the URL configured for the respective application. This mechanism ensures secure and authenticated interaction with the pipeline, maintaining the integrity of the data flow.

**Events Processing**

When events reach the API gateway, they are forwarded and stored in a DynamoDB table named RAW, which is responsible for storing all raw events. Streaming is also configured with this DynamoDB table, meaning that after storing the event, it triggers a Lambda function to send these events to an SNS Topic.

Several Lambda functions are subscribed to this SNS Topic. For instance, the Splunk normalization Lambda function handles and normalizes events originating from Splunk, while the Datadog normalization Lambda function deals with events from Datadog. 

Once the events reach the SNS Topic, it publishes these messages, and the respective Lambda functions receive the events they are subscribed to. Each Lambda function processes its designated events, normalizes them, and then stores them into a DynamoDB table named Normalized.

Streaming is also configured in the Normalized DynamoDB table to handle the flow of normalized events. This streaming mechanism forwards the normalized events to a Lambda function, which then sends the events to an SNS Topic. The SNS Topic publishes these events to its subscribers, one of which is the ServiceNow Send Lambda function.

The ServiceNow Send Lambda function receives these events and organizes them into the desired format specified for ServiceNow. It then sends these formatted events to the ServiceNow destination. Note: There are different URLs for the ServiceNow destination, similar to the different URLs for different environments. Additionally, after processing, these events are stored in the DynamoDB Send Table to indicate that they have been processed.We might receive the same event multiple times from an SNS topic or SQS. This could be due to Retry Policies, Network Issues, or concurrency.We perform idempotency checks in the lambda function to prevent processing the same event multiple times. Also, we use SQS to store events that failed to process before. This way, we can resend them to the ServiceNow Send Lambda function for processing.

**Referenced Images**

**Current State**

![Current State](current-state.png "Current State")

**Event Management Data Pipeline**

![Event Management Data Pipeline](pipeline-image-2.jpg "Event Management Data Pipeline")

**Useful Links**

##### Pipeline


| Environment | URL                      |  ThousandEyes           | Dynatrace                |
| ----------- | -------------------------|-------------------------| -------------------------|
| Dev         | `https://www.google.com` |`https://www.google.com` | `https://www.google.com` |
| Test        | `https://www.google.com` |`https://www.google.com` | `https://www.google.com` |
| Prod        | `https://www.google.com` |`https://www.google.com` | `https://www.google.com` |


##### ServiceNow 

| Environment | URL |
| ----------- | ----------------------- |
| Dev         | `https://www.google.com`|
| Test        | `https://www.google.com`|
| Prod        |`https://www.google.com` |
